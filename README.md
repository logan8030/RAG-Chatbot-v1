edwin_RAG/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/                # Place your source PDFs here
â”‚   â””â”€â”€ chunks/              # Generated JSONL chunks
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py           # PDF â†’ per-page text w/ metadata
â”‚   â”œâ”€â”€ chunk.py             # Text â†’ semantically chunked JSONL
â”‚   â”œâ”€â”€ embed_qdrant.py      # Chunked text â†’ Qdrant vector DB
â”‚   â”œâ”€â”€ query.py             # CLI: query chunks (no LLM)
â”‚   â”œâ”€â”€ chat.py              # CLI: query â†’ retrieve â†’ LLM
â”‚   â””â”€â”€ app.py               # FastAPI server for chat API
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py               # Streamlit web UI (will add below)
â”œâ”€â”€ docker-compose.yml       # Qdrant service
â””â”€â”€ requirements.txt


ğŸ§ª Individual Script Usage
--------------------------

--extract.py--
ğŸ“ Extract text and metadata from PDFs

bash
python src/extract.py

Input: data/pdfs/*.pdf
Output: data/chunks/raw_chunks.jsonl


--chunk.py--
âœ‚ï¸ Chunk long text into ~700-character segments

bash
python src/chunk.py

Input: raw_chunks.jsonl
Output: chunked.jsonl


--embed_qdrant.py--
ğŸ“Œ Embed and store chunks in Qdrant

bash
docker compose up -d   # Start Qdrant (if not already running)
python src/embed_qdrant.py


--query.py--
ğŸ” Test retrieval from Qdrant

bash
python src/query.py

Input: user question
Output: relevant document chunks (no LLM yet)

--chat.py--
ğŸ§  Run full RAG pipeline with local LLM (Ollama)

bash
ollama run mistral      # Seperate terminal, start Ollama LLM

bash
python src/chat.py

--ğŸŒ API Usage â€” app.py--
Starts FastAPI server with /chat endpoint

bash
uvicorn src.app:app --reload
ğŸ”— Swagger UI:
http://localhost:8000/docs

Example POST /chat:
bash
curl -X POST http://localhost:8000/chat \
     -H "Content-Type: application/json" \
     -d '{"query": "What are the eviction rules in the 2024 guidebook?"}'